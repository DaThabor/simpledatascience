{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](DaThabor_Logo.png)\n",
    "\n",
    "----------\n",
    "\n",
    "# MENTORING SESSIONS\n",
    "\n",
    "<br>\n",
    "\n",
    "May, 2020\n",
    "\n",
    "<br>\n",
    "\n",
    "## Data Science and Analytics\n",
    "\n",
    "<br>\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENT\n",
    "\n",
    "1. Exploratory Data Analysis\n",
    "2. Inferential Statistics\n",
    "3. Hypothesis Testing\n",
    "4. Linear Models\n",
    "5. Logistic Models\n",
    "6. Principal Component Analysis (PCA)\n",
    "7. Clustering\n",
    "8. Support Vector Machines (SVM)\n",
    "9. Trees (Random Forest, Decision Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "The first step in any Data Analytics or Data Science project, would be to explore the data. The aim is to extract as much information as possible to gain insights on the data. While doing Exploratory Data Analysis (EDA), information is prepared, data is cleaned, new derived columns are created and visualizations are used.\n",
    "\n",
    "<br>\n",
    "\n",
    "The following aspects will be done during the EDA phase:\n",
    "\n",
    "    Data Sourcing\n",
    "    Data Cleaning\n",
    "    Univariate Analysis\n",
    "    Bivariate Analysis\n",
    "    Derived Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sourcing\n",
    "\n",
    "In the beginning of your research you will be looking at 2 types of data:\n",
    "\n",
    "    Public data\n",
    "    Private data\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Public Data\n",
    "\n",
    "Any data (set) that is freely available on a public website, e.g. datagov.in, kaggle, google data sets, etc.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Private Data\n",
    "\n",
    "Any data that is restricted in use, e.g. company data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Data cleaning is required to make sure that there are no data quality issues, that may prevent you from running statistical models, or any algorithms. The main cleaing operations are:\n",
    "\n",
    "    Fixing Rows and Columns\n",
    "    (Treating) Missing values\n",
    "    Standardizing or normalizing data\n",
    "    Correct invalid or incomplete values\n",
    "    Filtering data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni- and Bi-variate Analysis\n",
    "\n",
    "After cleaning the data, the next step is to look at both univariate (one variable) and bi- and multi-variate (between 2 or multiple variables) analysis. For this you can plot some of the variables to get a better understanding and insight. The outcome of this analysis is also used for the next step in deriving additional information (created or retrieved from other sources)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Columns\n",
    "\n",
    "From the analysis some information might either be missing, or some additional information can be added to the data set to get even more or better quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Statistics\n",
    "\n",
    "Inferential Statistics are used to handle big amounts of data, and to be able to work with samples, in stead of the entire population. Which means that we infer information from smaller sets of the data, and then make estimations or assumptions about the total population.\n",
    "\n",
    "In inferential statistics we normally look at:\n",
    "\n",
    "    Probability Distributions\n",
    "    Central Limit Theorem\n",
    "    Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Based on the assumptions made on the sample space, we will use hypothesis testing to check those assumptions. For this we will use:\n",
    "\n",
    "    Null and Alternate Hyptohesis\n",
    "    Making a decision based on the hypothesis by:\n",
    "        Ctitical Value\n",
    "        P-value\n",
    "    Checking the error types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "Regression models are models where the output variable to be predicted is a continuous variable. Regression models are part of the supervised learning methods.\n",
    "\n",
    "A (simple) linear regression model attempts to explain the relationship between a dependent and an independent variable using a straight line.\n",
    "\n",
    "For this we try to found the 'best-fit-line', that fits the data set the best. The best-fit-line is found by looking at the residuals and Residual Sum of Squares (RSS). By minimizing the RSS for every possible line in the data set, we can find the best-fitted-line, where the RSS value is the lowest.\n",
    "\n",
    "Then based on this best-fit-line, we can predict any other value. By using either $R^2$ or Residual Square Error (RSE), we can check how well the prediction fitted to our model. The higher the squared value, the better the model is fitted, but be aware of the common pittfall of overfitting.\n",
    "\n",
    "[Simple Linear Regression in Python](http://localhost:8888/notebooks/Jupyter%20Lab%20Notebooks/MentoringClasses/Simple%20Linear%20Regression%20in%20Python.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Models\n",
    "\n",
    "For logistic models the output variable is a categorical variable, and therefore it is a classification model. \n",
    "\n",
    "In logistic regression we come to see about:\n",
    "\n",
    "        Binary classification\n",
    "        Sigmoid function\n",
    "        Likelihood function\n",
    "        Odds and log odds\n",
    "        Confusion Matrix and Accuracy\n",
    "        Sensitivity and specificity\n",
    "        ROC curve\n",
    "        Precision & Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is an unsupervised model and is mainly used when you have a large number of (potential) correlated variables. PCA is mainly used to get a better understanding of the variables in detail and making it possible to reduce dimensions of your data set to improve modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "The next unsupervised model is clustering. Clustering is mainly used on data for which we don't have an outcome variable. Based on the data, we try to determine if we can combine (cluster) data, as to find out specific clusters with similar features. The main model used here is the K-Means model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "SVM's are capable of dealing with quite complex problems, where models such as logistic regression typically fail. SVM's have been extensively used for solving complex classification problems such as image recognition, voice detection etc.\n",
    "\n",
    "SVM's are mostly used for classification tasks, but they can also be used for regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees (Randomg Forest & Decision Trees)\n",
    "\n",
    "The last part is focused on the unsupervised model for trees: decision trees and random forest. The data in these models is split into smaller parts one-by-one, using a tree structure. Each split in the tree can be tweaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
